\documentclass[final]{beamer}
\usepackage{grffile}
\mode<presentation>{\usetheme{CambridgeUSPOL}}

\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{array,booktabs,tabularx}
\usepackage{epstopdf}
\usepackage{colortbl, xcolor}
\newcolumntype{Z}{>{\centering\arraybackslash}X}

% rysunki
\usepackage{tikz}
\usepackage{ifthen}
\usepackage{xxcolor}
\usetikzlibrary{arrows}
\usetikzlibrary[topaths]
\usetikzlibrary{decorations.pathreplacing}
%\usepackage{times}\usefonttheme{professionalfonts}  % times is obsolete
\usefonttheme[onlymath]{serif}
\boldmath
\usepackage[orientation=portrait,size=a0,scale=1.4,debug]{beamerposter}                       % e.g. for DIN-A0 poster
%\usepackage[orientation=portrait,size=a1,scale=1.4,grid,debug]{beamerposter}                  % e.g. for DIN-A1 poster, with optional grid and debug output
%\usepackage[size=custom,width=200,height=120,scale=2,debug]{beamerposter}                     % e.g. for custom size poster
%\usepackage[orientation=portrait,size=a0,scale=1.0,printer=rwth-glossy-uv.df]{beamerposter}   % e.g. for DIN-A0 poster with rwth-glossy-uv printer check
% ...
%

\usecolortheme{seagull}
\useinnertheme{rectangles}
\setbeamercolor{item projected}{bg=darkred}
% \setbeamertemplate{enumerate items}[default]
\setbeamertemplate{caption}{\insertcaption} 
\setbeamertemplate{navigation symbols}{}
\setbeamercovered{transparent}
\setbeamercolor{block title}{fg=darkred}
\setbeamercolor{local structure}{fg=darkred}

\setbeamercolor*{enumerate item}{fg=darkred}
\setbeamercolor*{enumerate subitem}{fg=darkred}
\setbeamercolor*{enumerate subsubitem}{fg=darkred}

\setbeamercolor*{itemize item}{fg=darkred}
\setbeamercolor*{itemize subitem}{fg=darkred}
\setbeamercolor*{itemize subsubitem}{fg=darkred}

\newlength{\columnheight}
\setlength{\columnheight}{96cm}
\renewcommand{\thetable}{}
\def\andname{,}
\authornote{}

\renewcommand{\APACrefatitle}[2]{}
\renewcommand{\bibliographytypesize}{\footnotesize} 
\renewcommand{\APACrefYearMonthDay}[3]{%
  {\BBOP}{#1}
  {\BBCP}
}

\begin{document}

<<knitrIntro, echo = FALSE, message=FALSE,warning=FALSE>>=
source("my_ggplot_theme.R")

library(dplyr)
library(reshape2)
library(xtable)
library(biogram)

@


\date{}
\author{Micha\l{} Burdukiewicz\inst{1}, Piotr Sobczyk\inst{2} and Pawe\l{} Mackiewicz\inst{1}\\
\small{*michalburdukiewicz@gmail.com}}


\institute{\small{\textsuperscript{1}University of Wroc\l{}aw, Department of Genomics 

\textsuperscript{2}Wroc\l{}aw University of Science and Technology, Faculty of Pure and Applied Mathematics
}
}
}
\title{\huge biogram: standardization of biological n-gram analysis in R}

\begin{frame}
\begin{columns}
\begin{column}{.485\textwidth}
\begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
\begin{minipage}[T]{.95\textwidth}
\parbox[t][\columnheight]{\textwidth}
{
\begin{block}{Aim}

Create a tool to standardize analysis of n-grams extracted from biological sequences in the \textbf{R} environment.

\end{block}

\begin{block}{n-grams}
n-grams (k-tuples) are vectors of n characters derived from input sequence(s). They may form continuous sub-sequences or be discontinuous. 
Important n-gram parameter is its position. Instead of just counting n-grams, one may want to count how many n-grams occur at a given position in multiple (e.g. related) sequences.

Originally developed for natural language processing, n-grams are also used in genomics~\citep{fang2011}, transcriptomics~\citep{wang2014} and proteomics~\citep{guo2014}.


\small{
       \begin{columns}[c] % the "c" option specifies center vertical alignment
    \column{.5\textwidth} 
<<echo = FALSE,message=FALSE,results='asis'>>=
sample_seq <- matrix(sample(c("A", "C", "G", "T"), 18, replace = TRUE), nrow = 3)
colnames(sample_seq) <- paste0("P", 1L:ncol(sample_seq))
rownames(sample_seq) <- paste0("S", 1L:nrow(sample_seq))
rws <- seq(1, nrow(sample_seq) - 1, by = 2)
col <- rep("\\rowcolor{white}", length(rws))

print(xtable(data.frame(sample_seq), caption = "Sample sequences.  S - sequence, P - postion.", digits = 0),
      booktabs = TRUE, add.to.row = list(pos = as.list(rws), command = col))

# unis <- count_ngrams(sample_seq, 1, 1L:4, pos = TRUE)
# unis <- data.frame(as.matrix(unis))[, 1L:7]
# print(xtable(unis, caption = "A fraction of possible unigrams with position information.", digits = 0), include.rownames = FALSE)
@
      
      
     % column designated by a command

    \column{.5\textwidth}
    
<<echo = FALSE,message=FALSE,results='asis'>>=
    
     unis <- count_ngrams(sample_seq, 1, c("A", "C", "G", "T"))
unis <- data.frame(as.matrix(unis))
colnames(unis) <- c("A", "C", "G", "T")
rownames(unis) <- paste0("S", 1L:nrow(sample_seq))
print(xtable(unis, caption = "Unigram counts.", digits = 0), include.rownames = TRUE,
      booktabs = TRUE, add.to.row = list(pos = as.list(rws), command = col))
@

    \end{columns}

<<echo = FALSE,message=FALSE,results='asis'>>=
    
unis <- count_ngrams(sample_seq, 1, c("A", "C", "G", "T"), pos = TRUE)
unis <- data.frame(as.matrix(unis))[, 1L:13]
colnames(unis) <- substr(sub("X", "P", colnames(unis)), 0, 4)
rownames(unis) <- paste0("S", 1L:nrow(sample_seq))
print(xtable(unis, caption = "A fraction of possible unigrams with position information.", digits = 0), 
      include.rownames = TRUE,
      booktabs = TRUE, add.to.row = list(pos = as.list(rws), command = col))
@
}    
    \end{block}

 \begin{block}{n-gram name convention}
    n-gram names follow a specific convention and have three parts for position-specific n-grams and two parts otherwise. The parts are separated by _. The . symbol is used to separate elements within a part.
      
    \end{block}
    \vfill



 \begin{block}{Feature selecting permutation tests}
    Model and statistic independent permutation tests can be used to filter features obtained through counting n-grams.
    
    During a permutation test class labels are randomly exchanged during computation of a significance statistic. p-values are defined as:
    
\begin{center}
\scalebox{0.85}{
$      
\textnormal{p-value} = \frac{N_{T_P > T_R}}{N}
$
}
\end{center}

where $N_{T_P > T_R}$ is number of times when $T_P$ (permuted test statistic) was more extreme than $T_R$ (test statistic for non-permuted data).

Permutation tests are computationally expensive (especially considering precise estimation of small p-values, because the number of permutations is inversely proportional to the interval between p-values).
      
    \end{block}
    \vfill
    
    
\begin{block}{QuiPT concept}

In each permutation, for every observation, there are four possible results:

\begin{center}
\scalebox{0.85}{
$P(Target, Feature) = (1,1)) = p \cdot q$
}
\end{center}

\\

\begin{center}
\scalebox{0.85}{
$P(Target, Feature) = (1,0)) = p \cdot (1-q)$
}
\end{center}

\\

\begin{center}
\scalebox{0.85}{
$P(Target, Feature) = (0,1)) = (1-p) \cdot q$
}
\end{center}

\\

\begin{center}
\scalebox{0.85}{
$P(Target, Feature) = (0,0)) = (1-p) \cdot (1-q)$
}
\end{center}

\\

Where $p$ and $q$ are fractions of positive observations in target and
feature respectively. An another view at permutation test is therefore that we 
get a contingency table, which is to be tested for independence.
Computing probability of a such table with two constraints, $n_{1,\cdot} = n_{1,1} + n_{1,0}$ and
$n_{\cdot, 1} = n_{1,1} + n_{0,1}$, and 
conditioning on $n_{1,1}$, leads to hypergeometric distribution.
$n_{i,j}$ denotes number of observations for which 
\scalebox{0.85}{$(Target, Feature) = (i,j)$}

This is in fact exact two-sided Fisher's test~\citep{lehmann1986testing}. 
%Information Gain is a way of deciding how strong is the evidence against independance.

\end{block}
\vfill 



}
\end{minipage}
\end{beamercolorbox}
\end{column}


%new column ------------------------------------------------------    

\begin{column}{.51\textwidth}
\begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
\begin{minipage}[T]{.95\textwidth}  
\parbox[t][\columnheight]{\textwidth}
{


\begin{block}{Computational cost}

The cost of performing QuiPT is equal to computing a test statistic and 
probability of occurrence for $n_{1,1} + n_{0,1}$ contingency tables.

Suppose we consider 6-grams build on sequences of length 25 build of four 
characters. Then there are around 100,000 n-grams (features) to test. 
This means that for Benjamini-Hochberg procedure, we need to calculate 
p-values with accuracy of $0.05 \times 10^{-5}$. This requires at least 
2 million permutations. Each permutation, apart from reshuffling labels, 
requires computation of a test statistic. Since n-gram features are very sparse vectors, QuiPT needs to evaluate only few contingency tables.

The relative difference in speed between QuiPT and normal permutation tests depends on several factors, as a number of permutations and input data. For example, for simulation scheme presented below, QuiPT was on average 93 times faster than normal permutation test with $10^5$ permutations.

\end{block}
\vfill 


\begin{block}{Distribution of Information Gain for given contingency table}

\begin{columns}
\column{.04\textwidth}
\column{.35\textwidth}
\centering
Given constraint on $n_{1,1} + n_{0,1}$, probability distribution on
contingency tables, which permutations might produce, 
can be computed exactly.
\column{.65\textwidth}
\scalebox{0.79}{
  <<echo = FALSE,message=FALSE,fig.align='center',fig.width=13,fig.height=11>>=
  target_feature <- create_feature_target(40, 10, 25, 40)
  xyTable <- table(target_feature[, 1], target_feature[, 2])
  colnames(xyTable) <- c("Positive\nTarget", "Negative\nTarget")
  rownames(xyTable) <- c("Positive\nFeature", "Negative\nFeature")
  tmp <- distr_crit(target = target_feature[,1], feature = target_feature[,2])
  
  ggplot_distr <- function(x) {
b <- data.frame(cbind(x=as.numeric(rownames(attr(x, "plot_data"))),
                      attr(x, "plot_data")))
d1 <- cbind(b[,c(1,2)], attr(x, "nice_name"))
d2 <- cbind(b[,c(1,3)], "Probability")
colnames(d1) <- c("x", "y", "panel")
colnames(d2) <- c("x", "y", "panel")
d <- rbind(d1, d2)
p <- ggplot(data = d, mapping = aes(x = x, y = y)) +
  facet_grid(panel~., scale="free") +
  geom_freqpoly(data= d2, aes(color=y), stat = "identity") +
  scale_fill_brewer(palette = "Set1") +
  geom_point(data=d1, aes(size=y), stat = "identity") +
  guides(color = "none") +
  guides(size = "none") +
  xlab("Number of cases with feature=1 and target=1") + 
  ylab("") +
  my_theme
p
  }
  
  ggplot_distr(tmp)
  @
}
\end{columns}
\end{block}
\vfill

\begin{block}{Simulation scheme - genomics}

\begin{enumerate}[1.]
\item Random 4000 sequences (20 nucleotides each). The half of the sequences has label 0.
\item Choose a single position between 3 and 18 (to avoid border cases).
\item Resample nucleotides at chosen position. The dominant nucleotide has probability of occurrence $p_d = 0.25$. Other nucleotides have probability of occurrence $p_o = (1 - p_d)/3  $. 
\item Perform QuiPT (Information Gain as test statistic) and choose significant features (with p-value $< 0.001$).
\item Iterate steps 1-4 over other values of $p_d$ - 0.38, 0.51, 0.65, 0.78, 0.91.
\item Repeat steps 1-5 200 times.
\end{enumerate}
    
\end{block}
\vfill

\begin{block}{Power and False discoveries}
    \centering
\scalebox{0.9}{  
<<echo = FALSE,message=FALSE,fig.align='center', fig.width=15>>=
load("three_n_power.RData")
  
tfres <- function(x, cn = c("1-ngram", "2-gram", "3-gram"),
                  rn = round(seq(from = 0.25, to = 0.91, length.out = 6), 2)) {
  colnames(x) <- cn
  rownames(x) <- rn
  x <- melt(x)
  #significant proportion, ngram, value
  colnames(x) <- c("sig", "ngram", "value")
  x[["sig"]] <- as.factor(x[["sig"]])
  x
}

three_n_power <- three_n_power[sapply(three_n_power, function(i) i[[6]][[3]][2]) > 150]

power <- Reduce("+", lapply(three_n_power, function(single_rep) {
  #single replication dat
  sr_dat <- do.call(rbind, lapply(single_rep, unlist))
  sr_dat[, c(1, 3, 5)]/matrix(rep(c(4, 32, 192), 6), nrow = 6, byrow = TRUE)
}))/length(three_n_power)

mean_nonan <- function(x) {
  sum(x[!is.nan(x)])/sum(!is.nan(x))
  x[is.nan(x)] <- 0
  mean(x)
}

fs_dat <- sapply(three_n_power, function(single_rep) {
  #single replication dat
  sr_dat <- do.call(rbind, lapply(single_rep, unlist))
  (sr_dat[, c(2, 4, 6)] - sr_dat[, c(1, 3, 5)])/sr_dat[, c(2, 4, 6)]
})

fs_mean <- matrix(apply(fs_dat, 1, mean_nonan), nrow = 6)

#no significant features at all
no_sig <- matrix(apply(fs_dat, 1, function(i)
  sum(is.nan(i))), nrow = 6)

fs <- data.frame(tfres(fs_mean), 
                 no_sig = (length(three_n_power) - tfres(no_sig)[["value"]])/length(three_n_power),
                 power=tfres(power)$value)
#making signal strength number from range [0,1]
#fs$sig <- as.factor((as.numeric(fs$sig)-1)/5) 

p <- ggplot(fs, aes(x = ngram, y = sig, fill = power)) + 
  geom_tile() +
  scale_fill_continuous(name = "Power",
                        high = "#00CC00", low =  "#CCFFCC") +
  geom_point(aes(x = ngram, y = sig, size = value)) +
  scale_size_continuous("False significant \nfeature frequency") +
  ylab(expression(paste("Signal strength - ", p[d]))) + xlab("n-gram size") +
  my_theme
p + geom_abline(intercept=1.5, slope=0.0, color="#666666", size=5) 


@
}

    
    \end{block}
    \vfill



\begin{block}{Summary and funding}
The n-gram analysis creates versatile classifiers able to extract more universal decision rules (e.g. \textit{signalHsmm}, which is able to also predict signal peptides in atypical organisms) or better detect specific proteins. Nonetheless, despite computational quickness provided by the QuiPT method, curse of dimensionality limits n-gram methods to the analysis of shorter sequences.

\bigskip

Our software is avaible as web-servers:

\begin{enumerate}

\item \textit{signalHsmm} web-server: \url{smorfland.uni.wroc.pl/signalHsmm}.

\item \textit{AmyloGram} web-server: \url{smorfland.uni.wroc.pl/amylogram}.

\end{enumerate}

\bigskip

This research was partially funded by the KNOW Consortium and National Science Center (2015/17/N/NZ2/01845).

\end{block}
\vfill

 \begin{block}{Bibliography}
  \tiny{
  \bibliographystyle{apalike}
  \bibliography{references}
  }
  \end{block}
  \vfill  


}
\end{minipage}
\end{beamercolorbox}
\end{column}
\end{columns}  
\end{frame}
\end{document}